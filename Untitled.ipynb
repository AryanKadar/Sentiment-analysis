{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997efce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mp\n",
    "from sklearn.model_selection import GridSearchCV as GSCV,train_test_split as TTS\n",
    "from sklearn.compose import ColumnTransformer as CT\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import accuracy_score as AS\n",
    "import seaborn as sn\n",
    "from sklearn.pipeline import Pipeline as PP\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CVE\n",
    "from sklearn.ensemble import StackingClassifier as SC,VotingClassifier as VC,BaggingClassifier as BC\n",
    "from nltk.stem import WordNetLemmatizer as WNL\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet \n",
    "import string\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107f1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB as BNB,MultinomialNB as MNB\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e395103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Reviews  Sentiments\n",
       "0    A very, very, very slow-moving, aimless movie ...           0\n",
       "1    Not sure who was more lost - the flat characte...           0\n",
       "2    Attempting artiness with black & white and cle...           0\n",
       "3         Very little music or anything to speak of.             0\n",
       "4    The best scene in the movie was when Gerardo i...           1\n",
       "..                                                 ...         ...\n",
       "804  I just got bored watching Jessice Lange take h...           0\n",
       "805  Unfortunately, any virtue in this film's produ...           0\n",
       "806                   In a word, it is embarrassing.             0\n",
       "807                               Exceptionally bad!             0\n",
       "808  All in all its an insult to one's intelligence...           0\n",
       "\n",
       "[809 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=pd.read_csv(\"imdb_labelled.txt\",delimiter=\"\\t\",header=None,names=[\"Reviews\",\"Sentiments\"])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8ea4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag[0]=='J':\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag[0]=='V':\n",
    "        return wordnet.VERB   \n",
    "    elif treebank_tag[0]=='R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bd090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WNL()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t,pos=get_wordnet_pos(nltk.pos_tag(list(t))[0][1])) for t in word_tokenize(articles) if (t not in string.punctuation+\"``\"+\"...\"+\"--\") and (t.isnumeric()==False) and (\"'\" not in t) and ((\".\" not in t))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2993c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      A very, very, very slow-moving, aimless movie ...\n",
       "1      Not sure who was more lost - the flat characte...\n",
       "2      Attempting artiness with black & white and cle...\n",
       "3           Very little music or anything to speak of.  \n",
       "4      The best scene in the movie was when Gerardo i...\n",
       "                             ...                        \n",
       "804    I just got bored watching Jessice Lange take h...\n",
       "805    Unfortunately, any virtue in this film's produ...\n",
       "806                     In a word, it is embarrassing.  \n",
       "807                                 Exceptionally bad!  \n",
       "808    All in all its an insult to one's intelligence...\n",
       "Name: Reviews, Length: 809, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t[\"Reviews\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd69cc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "804    0\n",
       "805    0\n",
       "806    0\n",
       "807    0\n",
       "808    0\n",
       "Name: Sentiments, Length: 809, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=t[\"Sentiments\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a6ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "xta,xte,yta,yte=TTS(x,y,random_state=2,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffa5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=PP(\n",
    "[\n",
    "    (\"CVE\",CVE(strip_accents=\"ascii\",min_df=2,stop_words=\"english\",tokenizer=LemmaTokenizer())),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fd2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc=BC(estimator=MNB(),n_jobs=-1,n_estimators=1000,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f91aab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AryanBr0\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\AryanBr0\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7592592592592593"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp2=PP(\n",
    "[\n",
    "    (\"CVE\",ct),\n",
    "    (\"BC\",GSCV(bc,param_grid={},cv=5,scoring=\"accuracy\",n_jobs=-1))\n",
    "])\n",
    "pp2.fit(xta,yta)\n",
    "AS(yte,pp2.predict(xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b223cead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7434704830053667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp2.named_steps[\"BC\"].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612eec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def greet(Sentiment):\n",
    "    if pp2.predict([Sentiment])[0]==1:\n",
    "        return \"Happy\"\n",
    "    else:\n",
    "        return \"Not Happy\"\n",
    "grr=gr.Interface(fn=greet,inputs=\"text\",outputs=\"text\")\n",
    "grr.launch();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042e893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd35740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
